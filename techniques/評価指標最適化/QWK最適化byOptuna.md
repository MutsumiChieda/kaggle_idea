# Optuna を使って QWK の閾値を最適化してみる

[コード付き解説記事](https://blog.amedama.jp/entry/optuna-qwk-optimization)

一般的には，`scipy.optimize.minimize()`関数を使ってNelder Mead法で最適化する．

QWKの最適化は、真のラベルの分布に依存している．  
そのため、学習済みモデルを使って予測する対象が学習時の分布と異なる場合，最適化の効果が得られない．  
この点から、Private データの分布が異なる場合には最適化が有効ではない．

Private データの分布が学習時と同じであるという確証が得られない限り，使うのは賭けになる．  
一方で，うまくいった場合の効果は絶大なので，ある程度の仮定のもとに使うべき．

## 精度比較

一般的に回帰の方が精度出るらしい．

|手法|精度|
|-|-|-|
多値分類 | 0.5574  
回帰+四捨五入 | 0.6064
回帰+OOF最適化 | 0.6461