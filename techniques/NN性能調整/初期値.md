# ニューラルネットワークの初期値

ニューラルネットワークの重みに与える初期値．  
勾配消失のしにくさに影響する．

活性化関数|初期値|名称
|-|-|-|
ReLU|$\sqrt{\frac{3}{n}}$|LeCun
ReLU|$\sqrt{\frac{2}{n}}$|He
sigmoid|$\frac{1}{\sqrt{n}}$|Xavier
tanh|$\frac{1}{\sqrt{n}}$|Xavier

またInitializerのハイパーパラメータscaleは小さめに設定する．

## PyTorch向けコード

```python  
def weight_init(m):
    if isinstance(m, nn.Conv2d):
        # nn.init.xavier_normal_(m.weight.data)  # Xavier
        # nn.init.kaiming_normal_(m.weight.data) # He

        # for bias term, initialize w/ constant
        if m.bias is not None: 
            nn.init.constant_(m.bias, 0.0)

# assuming net is nn.Module
net.apply(weight_init)
```  
